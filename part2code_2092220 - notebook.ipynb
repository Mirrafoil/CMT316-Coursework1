{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outdoor-disclaimer",
   "metadata": {},
   "source": [
    "# Jupyter Notebook to accompany Part 2 of Coursework 1\n",
    "\n",
    "## Peter Kennedy - 2092220"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-richmond",
   "metadata": {},
   "source": [
    "## Section 1 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expanded-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Block 1 - Importing and configuring libraries\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "import operator\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import spacy\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords.add(\".\")\n",
    "stopwords.add(\",\")\n",
    "stopwords.add(\"--\")\n",
    "stopwords.add(\"-\")\n",
    "stopwords.add(\"``\")\n",
    "stopwords.add(\"''\")\n",
    "stopwords.add(\"(\")\n",
    "stopwords.add(\")\")\n",
    "stopwords.add(\"%\")\n",
    "stopwords.add(\":\")\n",
    "stopwords.add(\"'\")\n",
    "stopwords.add(\"'s\")\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-harvey",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "The following functions are used within the 'main' functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entire-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Block 2 - Defenition of helper functions\n",
    "\n",
    "# Input: name of folder in which articles are searched for\n",
    "# Output: a list of labelled articles. The label given to each article is given by the name of its parent\n",
    "#   folder.\n",
    "# Remarks: Code modified from https://stackoverflow.com/questions/19587118/iterating-through-directories-with-python\n",
    "def get_article_data(folder):\n",
    "    rootdir = './datasets_coursework1/' + folder\n",
    "    article_data = []\n",
    "\n",
    "    for _, dirs, _ in os.walk(rootdir):\n",
    "        for category in dirs:\n",
    "            for subdir, _, articles in os.walk(os.path.join(rootdir, category)):\n",
    "                for article_path in articles:\n",
    "                    artile_text = open(os.path.join(subdir, article_path)).read()\n",
    "                    article_data.append((artile_text, category))\n",
    "                    \n",
    "    return article_data  \n",
    "\n",
    "# Input: article data which has been constructed in the format given in CB2.\n",
    "# Returns: a list of categories from list of labelled articles.\n",
    "def get_categories(article_data):\n",
    "    categories = []\n",
    "    for article in article_data:\n",
    "        if article[1] not in categories:\n",
    "            categories.append(article[1])\n",
    "    return categories\n",
    "\n",
    "\n",
    "# Input: a string\n",
    "# Returns a list of lemmitsed, lower case tokens from a string\n",
    "def get_lemmatized_tokens(string):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_tokens_list = []\n",
    "    tokens=nltk.tokenize.word_tokenize(string)\n",
    "    for token in tokens:\n",
    "        lemmatized_tokens_list.append(lemmatizer.lemmatize(token).lower())\n",
    "    return lemmatized_tokens_list\n",
    "\n",
    "\n",
    "# Input: a vocabulary of words and a string\n",
    "# Returns: vector which corresponds to the number of occurences of each word in the vocabulary\n",
    "def get_vector_text(vocab, string):\n",
    "    vector_text = np.zeros(len(vocab))\n",
    "    list_tokens_string= get_lemmatized_tokens(string)\n",
    "    for i, word in enumerate(vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i]=list_tokens_string.count(word)\n",
    "    return vector_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-disclosure",
   "metadata": {},
   "source": [
    "## Main functions\n",
    "\n",
    "The following functions are the main functions which go to construct vocabularies, and vectorise strings in order to use them to train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "willing-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Block 3 - Main functions\n",
    "\n",
    "# Input: a subset of article data which has been constructed in the format given in CB2.\n",
    "# Output: a list of article lengths\n",
    "def get_article_lengths(article_data, categories):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for article in article_data:\n",
    "        X.append(len(article[0]))\n",
    "        Y.append(np.where(np.array(categories) == article[1])[0][0]) # Assigns a numerical label to the category\n",
    "    \n",
    "    return np.array(X).reshape(-1,1), np.array(Y)\n",
    "        \n",
    "\n",
    "# Input: a subset of article data which has been constructed in the format given in CB2 and a vocabulary length.\n",
    "# Output: a list of the most commonly occuring words in the article data of length defined in the input\n",
    "def get_simple_vocabulary(article_data, vocab_length):\n",
    "\n",
    "    dict_word_frequency={}\n",
    "    for article in article_data:\n",
    "        tokens = get_lemmatized_tokens(article[0])\n",
    "        for token in tokens:\n",
    "            if token in stopwords: continue\n",
    "            if token not in dict_word_frequency: dict_word_frequency[token] = 1\n",
    "            else: dict_word_frequency[token] += 1\n",
    "\n",
    "    sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:vocab_length]\n",
    "\n",
    "    vocabulary=[]\n",
    "    for word,frequency in sorted_list:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "# Input: a subset of article data which has been constructed in the format given in CB2, an array of POS\n",
    "#   tags and a categorical vocabulary length.\n",
    "# Output: a list of the most commonly occuring words which have an allowed POS tag associated with it,\n",
    "#   taken from the article data. The length of the vocabulary is the categorical vocabulary length (taken\n",
    "#   as an input) multiplied by the number of categories.\n",
    "# Remarks: This vocabulary is constructed in a slightly different way than the simple vocabulary. To ensure that\n",
    "#   each category contributes to the vocabulary proportionally, a vocabulary for each category is first\n",
    "#   constructed (which has a length equal to the desired final vocab length, divided by the number of categories).\n",
    "#   Once this is done, these vocabularies are combined into an overall vocabulary. If duplicate words are found\n",
    "#   to exist in this, then they are 'squashed' out (duplicates removed). The process is then repeated to ensure\n",
    "#   that the final vocabulary is of the desired length.\n",
    "def get_categorical_pos_vocabulary(article_data, pos_array, cat_vocab_length):\n",
    "\n",
    "    category_list = get_categories(article_data)\n",
    "    vocab_length = cat_vocab_length * len(category_list)\n",
    "    vocab_to_add = cat_vocab_length * len(category_list)\n",
    "    vocabulary = []\n",
    "    \n",
    "    # Continues loop till vocabulary reaches desired length after duplicates have been squashed\n",
    "    while vocab_length > len(vocabulary):\n",
    "\n",
    "        categorical_vocabularies = []\n",
    "\n",
    "        for category in category_list:\n",
    "\n",
    "            # Count occurences of words which aren't in the stopwords list and sort in frequency order\n",
    "            dict_word_frequency={}\n",
    "            for article in article_data:\n",
    "                if article[1] == category:\n",
    "                    doc = spacy_nlp(f'{article[0]}')\n",
    "                    for token in doc:\n",
    "                        if token.pos_ in pos_array:\n",
    "                            if token.text in stopwords: continue\n",
    "                            if token.text in vocabulary: continue\n",
    "                            if token.text not in dict_word_frequency: dict_word_frequency[token.text] = 1\n",
    "                            else: dict_word_frequency[token.text] += 1\n",
    "\n",
    "            sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:math.ceil(vocab_to_add/len(category_list))]\n",
    "\n",
    "            # Create a vocabulary for a given category\n",
    "            categorical_vocabulary=[]\n",
    "            for word,frequency in sorted_list:\n",
    "                categorical_vocabulary.append(word)\n",
    "\n",
    "            categorical_vocabularies.append(categorical_vocabulary)\n",
    "\n",
    "        # Add to vocabulary\n",
    "        for cat_vocab in categorical_vocabularies:\n",
    "            for word in cat_vocab:\n",
    "                vocabulary.append(word)\n",
    "\n",
    "        # Squashes duplicate entries in vocabulary\n",
    "        vocab_set = set(vocabulary)\n",
    "        vocabulary = list(vocab_set)\n",
    "        \n",
    "        vocab_to_add = vocab_length - len(vocabulary)\n",
    "        \n",
    "        # Used to keep track of function progress\n",
    "        print(\"Vocab length: \" + str(len(vocabulary)))\n",
    "        print(\"Vocab to add: \" + str(vocab_to_add))\n",
    "        \n",
    "    if len(vocabulary) > vocab_length:\n",
    "        vocabulary = vocabulary[:vocab_length]\n",
    "        \n",
    "    return vocabulary\n",
    "    \n",
    "\n",
    "# Input: an ordered list of categories, article data, and a vocabulary of words\n",
    "# Output: a pair of numpy arrays. The first (X) is a vectorised form of the articles each of which have \n",
    "#   dimensionality of the length of the vocabulary. the second (Y) is a 1-dimensional array which simply\n",
    "#   assigns a numerical value based on the position of the label in the categories array.\n",
    "def xy_vector_split(categories, article_data, vocabulary):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for article in article_data:\n",
    "        vector_article = get_vector_text(vocabulary, article[0])\n",
    "        X.append(vector_article)\n",
    "        Y.append(np.where(np.array(categories) == article[1])[0][0]) # Assigns a numerical label to the category\n",
    "            \n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-billy",
   "metadata": {},
   "source": [
    "# Section 2 - Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-cream",
   "metadata": {},
   "source": [
    "## Parameters for experimentation with cross validation\n",
    "\n",
    "Set the parameters in the CB4 and run CB5 to experiment with different parameters using k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "speaking-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code Block 4 - Parameters for experimentation with cross validation\n",
    "\n",
    "################################# Setting up cross validation #################################\n",
    "\n",
    "folds = 10\n",
    "\n",
    "article_data = get_article_data('bbc/')\n",
    "\n",
    "categories = get_categories(article_data)\n",
    "\n",
    "################################# Feature extraction #################################\n",
    "\n",
    "# To set the features you would like to extract, use 'Length', 'Simple' or 'POS' to extract the article length,\n",
    "#  simple vocabulary or POS vocabulatry respectively.\n",
    "features = 'POS'\n",
    "\n",
    "# The vocabulary length is the total length of the vocabulary which is to be used for when the simple and POS\n",
    "#  vocabularies are being constructed as features.\n",
    "vocab_length = 3000\n",
    "\n",
    "\n",
    "### The following code (currently commented out) was used to construct all possible combinations of POS.\n",
    "### These combinations were then used to define the 'pos_combos' array, which CB will iterate through\n",
    "### by default, so that a comparison between different combinations of POS may be made.\n",
    "# pos = ['ADJ', 'ADV', 'INTJ', 'NOUN', 'PROPN', 'VERB']\n",
    "# pos_combos = []\n",
    "# for r in range(1, len(pos) + 1):\n",
    "#     pos_combos += list(combinations(pos, r))\n",
    "\n",
    "if features == 'POS':\n",
    "    pos_combos = [['NOUN', 'PROPN']] # Use this to set the parts of speech to be added to the vocabulary\n",
    "else:                                #  if features is set to 'POS'\n",
    "    pos_combos = [[]]\n",
    "\n",
    "################################# Feature Engineering #################################\n",
    "\n",
    "# Set to True if you would like to extract the best k features using the chi^2 comparison\n",
    "set_k_best = True\n",
    "# k_best must be less than or equal to the vocab_length\n",
    "k_best = 1500\n",
    "\n",
    "# Set to True if you would like to normalise the article vectors\n",
    "set_normalise = False\n",
    "\n",
    "# Set to True if you would like to apply the TF-IDF transformation to the article vectors\n",
    "set_tfidf = True\n",
    "\n",
    "svm_kernel = 'linear'\n",
    "\n",
    "# time format defenition used to keep track of timings during experimentation\n",
    "time_format = \"%H:%M:%S %d/%m/%Y\"\n",
    "\n",
    "error = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-outside",
   "metadata": {},
   "source": [
    "## Running experimentation with cross validation\n",
    "\n",
    "Results will be printed below the code block as it runs, and once it is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "handled-debut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using POS vocabulary with ['NOUN', 'PROPN'] tags\n",
      "Start time: 23:44:24 19/04/2021\n",
      "\n",
      "Folding fold 1/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2171\n",
      "Vocab to add: 829\n",
      "Vocab length: 2969\n",
      "Vocab to add: 31\n",
      "Vocab length: 3004\n",
      "Vocab to add: -4\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 1/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 1/10: 0.9483375959079284\n",
      "Precision of fold 1/10: 0.9507948715355304\n",
      "Recall of fold 1/10: 0.9460189950505983\n",
      "F1-Score of fold 1/10: 0.9479495711023436\n",
      "\n",
      "Folding fold 2/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2193\n",
      "Vocab to add: 807\n",
      "Vocab length: 2961\n",
      "Vocab to add: 39\n",
      "Vocab length: 3001\n",
      "Vocab to add: -1\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 2/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 2/10: 0.9432225063938618\n",
      "Precision of fold 2/10: 0.9464999568105789\n",
      "Recall of fold 2/10: 0.9407063075489454\n",
      "F1-Score of fold 2/10: 0.9430452762687761\n",
      "\n",
      "Folding fold 3/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2198\n",
      "Vocab to add: 802\n",
      "Vocab length: 2963\n",
      "Vocab to add: 37\n",
      "Vocab length: 3002\n",
      "Vocab to add: -2\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 3/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 3/10: 0.9447570332480818\n",
      "Precision of fold 3/10: 0.9467349943485918\n",
      "Recall of fold 3/10: 0.9418343077655061\n",
      "F1-Score of fold 3/10: 0.9438559966713793\n",
      "\n",
      "Folding fold 4/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2191\n",
      "Vocab to add: 809\n",
      "Vocab length: 2967\n",
      "Vocab to add: 33\n",
      "Vocab length: 3002\n",
      "Vocab to add: -2\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 4/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 4/10: 0.9442455242966752\n",
      "Precision of fold 4/10: 0.9477995376587185\n",
      "Recall of fold 4/10: 0.9415113157676493\n",
      "F1-Score of fold 4/10: 0.9439183124480671\n",
      "\n",
      "Folding fold 5/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2198\n",
      "Vocab to add: 802\n",
      "Vocab length: 2971\n",
      "Vocab to add: 29\n",
      "Vocab length: 3001\n",
      "Vocab to add: -1\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 5/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 5/10: 0.9437340153452686\n",
      "Precision of fold 5/10: 0.9464666134608025\n",
      "Recall of fold 5/10: 0.9408234136694797\n",
      "F1-Score of fold 5/10: 0.943055140451988\n",
      "\n",
      "Folding fold 6/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2197\n",
      "Vocab to add: 803\n",
      "Vocab length: 2968\n",
      "Vocab to add: 32\n",
      "Vocab length: 3003\n",
      "Vocab to add: -3\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 6/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 6/10: 0.9432225063938618\n",
      "Precision of fold 6/10: 0.9442416770538514\n",
      "Recall of fold 6/10: 0.940217281550795\n",
      "F1-Score of fold 6/10: 0.94190133633248\n",
      "\n",
      "Folding fold 7/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2196\n",
      "Vocab to add: 804\n",
      "Vocab length: 2968\n",
      "Vocab to add: 32\n",
      "Vocab length: 3003\n",
      "Vocab to add: -3\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 7/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 7/10: 0.9432225063938618\n",
      "Precision of fold 7/10: 0.945383713482879\n",
      "Recall of fold 7/10: 0.9408404923450249\n",
      "F1-Score of fold 7/10: 0.9426847057967184\n",
      "\n",
      "Folding fold 8/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2184\n",
      "Vocab to add: 816\n",
      "Vocab length: 2967\n",
      "Vocab to add: 33\n",
      "Vocab length: 3002\n",
      "Vocab to add: -2\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 8/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 8/10: 0.9360613810741688\n",
      "Precision of fold 8/10: 0.9399169436783715\n",
      "Recall of fold 8/10: 0.9330583883651873\n",
      "F1-Score of fold 8/10: 0.9356631314470487\n",
      "\n",
      "Folding fold 9/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2197\n",
      "Vocab to add: 803\n",
      "Vocab length: 2956\n",
      "Vocab to add: 44\n",
      "Vocab length: 3001\n",
      "Vocab to add: -1\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 9/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 9/10: 0.9401534526854219\n",
      "Precision of fold 9/10: 0.9422450273337304\n",
      "Recall of fold 9/10: 0.9377336107136891\n",
      "F1-Score of fold 9/10: 0.9395492277812358\n",
      "\n",
      "Folding fold 10/10...\n",
      "Generating POS vocabulary...\n",
      "Vocab length: 2193\n",
      "Vocab to add: 807\n",
      "Vocab length: 2957\n",
      "Vocab to add: 43\n",
      "Vocab length: 3001\n",
      "Vocab to add: -1\n",
      "Generating XY vector split...\n",
      "XY vector split for fold 10/10 complete\n",
      "Selecting best 1500 features...\n",
      "Applying TF-IDF transformation...\n",
      "Fitting SVM with linear kernel...\n",
      "Accuracy of fold 10/10: 0.9447570332480818\n",
      "Precision of fold 10/10: 0.9478083840697542\n",
      "Recall of fold 10/10: 0.9423591945903448\n",
      "F1-Score of fold 10/10: 0.944613755998958\n",
      "\n",
      "Average accuracy: 0.9431713554987212\n",
      "Average precision: 0.9457891719432807\n",
      "Average recall: 0.940510330736722\n",
      "Average F1-score: 0.9426236454298994\n",
      "\n",
      "End time: 23:58:40 19/04/2021\n",
      "Time taken: 0:14:15.550067\n",
      "\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Code Block 5 - Running experimentation with cross validation\n",
    "\n",
    "# Fold parameters\n",
    "kf = KFold(n_splits = folds)\n",
    "random.shuffle(article_data)\n",
    "kf.get_n_splits(article_data)\n",
    "\n",
    "for pos_array in pos_combos:\n",
    "\n",
    "    fold = 1\n",
    "    accuracy_total = 0\n",
    "    precision_total = 0\n",
    "    recall_total = 0\n",
    "    f1_total = 0\n",
    "\n",
    "    if features == 'Length':\n",
    "        print(\"Training using article length\")\n",
    "    elif features == 'Simple':\n",
    "        print(\"Training using simple vocabulary\")\n",
    "    elif features == 'POS':\n",
    "        print(\"Training using POS vocabulary with \" + str(pos_array) + \" tags\")\n",
    "    else:\n",
    "        print(\"Error: Please enter a valid feature to extract in the parameters section above (line 15 of CB4)\")\n",
    "        break\n",
    "        \n",
    "    if set_k_best and features != 'Length':\n",
    "        if k_best > vocab_length:\n",
    "            print(\"Error: the number of features to be extracted (line 40 of CB4) must be less than the vocabulary length (line 19 of CB4)\")\n",
    "            break\n",
    "\n",
    "    # Start the timer to keep track of how long K-fold validation of the experiment takes\n",
    "    start = datetime.now()\n",
    "    start_string = start.strftime(time_format)\n",
    "    print(\"Start time: \" + str(start_string) + \"\\n\")\n",
    "    \n",
    "    # Begin K-fold validation\n",
    "    for train_index, test_index in kf.split(article_data[:300]):\n",
    "        \n",
    "        # Construct a training and test set for the current fold\n",
    "        train_set_fold = []\n",
    "        test_set_fold = []\n",
    "        \n",
    "        print(\"Folding fold \" + str(fold) + \"/\" + str(folds) + \"...\")\n",
    "        for i,instance in enumerate(article_data):\n",
    "            if i in train_index:\n",
    "                train_set_fold.append(instance)\n",
    "            else:\n",
    "                test_set_fold.append(instance)\n",
    "        \n",
    "        if features == 'Length':\n",
    "            \n",
    "            # Check the length of the article\n",
    "            print(\"Checking lengths of articles...\")\n",
    "            X_train_fold, Y_train_fold = get_article_lengths(train_set_fold, categories)\n",
    "            X_test_fold, Y_test_fold = get_article_lengths(test_set_fold, categories)\n",
    "        \n",
    "        elif features == 'Simple' or features == 'POS':\n",
    "        \n",
    "            # Construct a simple or POS vocabulary depending on which parameters are set\n",
    "            if features == 'Simple':\n",
    "                print(\"Generating simple vocabulary...\")\n",
    "                vocab_fold = get_simple_vocabulary(train_set_fold, vocab_length)\n",
    "            elif features == 'POS':\n",
    "                print(\"Generating POS vocabulary...\")\n",
    "                vocab_fold = get_categorical_pos_vocabulary(train_set_fold, pos_array, math.floor(vocab_length/len(categories)))\n",
    "\n",
    "            print(\"Generating XY vector split...\")\n",
    "            X_train_fold, Y_train_fold = xy_vector_split(categories, train_set_fold, vocab_fold)\n",
    "            X_test_fold, Y_test_fold = xy_vector_split(categories, test_set_fold, vocab_fold)\n",
    "            print(\"XY vector split for fold \" + str(fold) + \"/\" + str(folds) + \" complete\")\n",
    "\n",
    "            if set_k_best:\n",
    "                print(\"Selecting best \" + str(k_best) + \" features...\")\n",
    "                features_fold = SelectKBest(chi2, k = k_best).fit(X_train_fold, Y_train_fold)\n",
    "                X_train_fold = features_fold.transform(X_train_fold)\n",
    "                X_test_fold = features_fold.transform(X_test_fold)\n",
    "\n",
    "            if set_tfidf:\n",
    "                print(\"Applying TF-IDF transformation...\")\n",
    "                tfidf = TfidfTransformer()\n",
    "                X_train_fold = tfidf.fit_transform(X_train_fold)\n",
    "                X_test_fold = tfidf.fit_transform(X_test_fold)\n",
    "            \n",
    "            # Since the TF-IDF transformer includes normalisation, the follwing code is only executed if \n",
    "            #  set_tfidf is False, and if set_normalise is True.\n",
    "            elif set_normalise:\n",
    "                print(\"Normalising vectors...\")\n",
    "                sc = StandardScaler()\n",
    "                X_train_fold = sc.fit_transform(X_train_fold)\n",
    "                X_test_fold = sc.fit_transform(X_test_fold)\n",
    "\n",
    "        print(\"Fitting SVM with \" + str(svm_kernel) + \" kernel...\")\n",
    "        svm_clf_fold = sklearn.svm.SVC(kernel=svm_kernel, gamma='auto')\n",
    "        svm_clf_fold.fit(X_train_fold, Y_train_fold)\n",
    "\n",
    "        Y_test_predictions_fold = svm_clf_fold.predict(X_test_fold)\n",
    "\n",
    "        accuracy_fold = accuracy_score(Y_test_fold, Y_test_predictions_fold)\n",
    "        precision_fold = precision_score(Y_test_fold, Y_test_predictions_fold, average='macro')\n",
    "        recall_fold = recall_score(Y_test_fold, Y_test_predictions_fold, average='macro')\n",
    "        f1_fold = f1_score(Y_test_fold, Y_test_predictions_fold, average='macro')\n",
    "        \n",
    "        print(\"Accuracy of fold \" + str(fold) + \"/\" + str(folds) + \": \" + str(accuracy_fold))\n",
    "        print(\"Precision of fold \" + str(fold) + \"/\" + str(folds) + \": \" + str(precision_fold))\n",
    "        print(\"Recall of fold \" + str(fold) + \"/\" + str(folds) + \": \" + str(recall_fold))\n",
    "        print(\"F1-Score of fold \" + str(fold) + \"/\" + str(folds) + \": \" + str(f1_fold) + \"\\n\")\n",
    "\n",
    "        accuracy_total += accuracy_fold\n",
    "        precision_total += precision_fold\n",
    "        recall_total += recall_fold\n",
    "        f1_total += f1_fold\n",
    "        fold += 1\n",
    "\n",
    "    accuracy_average = accuracy_total/folds\n",
    "    precision_average = precision_total/folds\n",
    "    recall_average = recall_total/folds\n",
    "    f1_average = f1_total/folds\n",
    "\n",
    "    print(\"Average accuracy: \" + str(accuracy_average))\n",
    "    print(\"Average precision: \" + str(precision_average))\n",
    "    print(\"Average recall: \" + str(recall_average))\n",
    "    print(\"Average F1-score: \" + str(f1_average) + \"\\n\")\n",
    "\n",
    "    end = datetime.now()\n",
    "    end_string = end.strftime(time_format)\n",
    "    print(\"End time: \" + str(end_string))\n",
    "    time_taken = end - start\n",
    "    print(\"Time taken: \" + str(time_taken))\n",
    "\n",
    "    print(\"\\n----------------------------------------\\n----------------------------------------\\n\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-chocolate",
   "metadata": {},
   "source": [
    "## Parameters for full dataset training\n",
    "\n",
    "Set the parameters in CB6 and run CB7 to train a classifier which is trained on the entire BBC news article dataset.\n",
    "\n",
    "This classifier may then be used to classify previously unseen news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "collect-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code block 6 - Parameters for full dataset training\n",
    "\n",
    "article_data = get_article_data('bbc/')\n",
    "\n",
    "categories = get_categories(article_data)\n",
    "\n",
    "################################# Feature extraction #################################\n",
    "\n",
    "# To set the features you would like to extract, use 'Length', 'Simple' or 'POS' to extract the article length,\n",
    "#  simple vocabulary or POS vocabulatry respectively.\n",
    "features = 'POS'\n",
    "\n",
    "# The vocabulary length is the total length of the vocabulary which is to be used for when the simple and POS\n",
    "#  vocabularies are being constructed as features.\n",
    "vocab_length = 3000\n",
    "\n",
    "# Use this to set the parts of speech to be added to the vocabulary\n",
    "pos_combos = ['NOUN', 'PROPN']\n",
    "\n",
    "################################# Feature Engineering #################################\n",
    "    \n",
    "# Set to True if you would like to extract the best k features using the chi^2 comparison\n",
    "set_k_best = True\n",
    "# k_best must be less than or equal to the vocab_length\n",
    "k_best = 1500\n",
    "\n",
    "# Set to True if you would like to normalise the article vectors\n",
    "set_normalise = False\n",
    "\n",
    "# Set to True if you would like to apply the TF-IDF transformation to the article vectors\n",
    "set_tfidf = True\n",
    "\n",
    "svm_kernel = 'linear'\n",
    "\n",
    "# time format defenition used to keep track of timings during experimentation\n",
    "time_format = \"%H:%M:%S %d/%m/%Y\"\n",
    "\n",
    "error = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-heart",
   "metadata": {},
   "source": [
    "## Training using full entire dataset\n",
    "\n",
    "This code block can be used to train a classifier on the entire BBC news article dataset, based on the best combinations of parameters that are found in the experimentation block above.\n",
    "\n",
    "This model may then be used to classify completely unseen articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "liked-vatican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using simple vocabulary\n",
      "Start time: 21:24:01 19/04/2021\n",
      "\n",
      "Generating simple vocabulary...\n",
      "Generating XY vector split...\n",
      "XY vector split complete\n",
      "Selecting best 2000 features...\n",
      "Normalising vectors...\n",
      "Fitting SVM with linear kernel...\n",
      "End time: 21:24:53 19/04/2021\n",
      "Time taken: 0:00:51.788853\n",
      "The model 'svm_clf' may now be used to make new predictions\n"
     ]
    }
   ],
   "source": [
    "### Code block 7 - Training using entire dataset\n",
    "\n",
    "if features == 'Length':\n",
    "    print(\"Training using article length\")\n",
    "elif features == 'Simple':\n",
    "    print(\"Training using simple vocabulary\")\n",
    "elif features == 'POS':\n",
    "    print(\"Training using POS vocabulary with \" + str(pos_array) + \" tags\")\n",
    "else:\n",
    "    print(\"Error: Please enter a valid feature to extract in the parameters section above (line # of CB #)\")\n",
    "    error = True\n",
    "\n",
    "if set_k_best and features != 'Length':\n",
    "    if k_best > vocab_length:\n",
    "        print(\"Error: the number of features to be extracted must be less than the vocabulary length\")\n",
    "        error = True\n",
    "\n",
    "if error == False:\n",
    "        \n",
    "    # Start the timer to keep track of how long K-fold validation of the experiment takes\n",
    "    start = datetime.now()\n",
    "    start_string = start.strftime(time_format)\n",
    "    print(\"Start time: \" + str(start_string) + \"\\n\")\n",
    "\n",
    "    if features == 'Length':\n",
    "\n",
    "        # Check the length of the article\n",
    "        print(\"Checking lengths of articles...\")\n",
    "        X_train, Y_train = get_article_lengths(article_data, categories)\n",
    "\n",
    "    elif features == 'Simple' or features == 'POS':\n",
    "\n",
    "        # Construct a simple or POS vocabulary depending on which parameters are set\n",
    "        if features == 'Simple':\n",
    "            print(\"Generating simple vocabulary...\")\n",
    "            vocab = get_simple_vocabulary(article_data, vocab_length)\n",
    "        elif features == 'POS':\n",
    "            print(\"Generating POS vocabulary...\")\n",
    "            vocab = get_categorical_pos_vocabulary(article_data, pos_array, math.floor(vocab_length/len(categories)))\n",
    "\n",
    "        print(\"Generating XY vector split...\")\n",
    "        X_train, Y_train = xy_vector_split(categories, article_data, vocab)\n",
    "        print(\"XY vector split complete\")\n",
    "\n",
    "        if set_k_best:\n",
    "            print(\"Selecting best \" + str(k_best) + \" features...\")\n",
    "            features = SelectKBest(chi2, k = k_best).fit(X_train, Y_train)\n",
    "            X_train = features.transform(X_train)\n",
    "\n",
    "            \n",
    "        if set_tfidf:\n",
    "            print(\"Applying TF-IDF transformation...\")\n",
    "            tfidf = TfidfTransformer()\n",
    "            X_train = tfidf.fit_transform(X_train)\n",
    "\n",
    "        if set_normalise:\n",
    "            print(\"Normalising vectors...\")\n",
    "            sc = StandardScaler()\n",
    "            X_train = sc.fit_transform(X_train)\n",
    "\n",
    "    print(\"Fitting SVM with \" + str(svm_kernel) + \" kernel...\")\n",
    "    svm_clf = sklearn.svm.SVC(kernel=svm_kernel, gamma='auto')\n",
    "    svm_clf.fit(X_train, Y_train)\n",
    "\n",
    "    end = datetime.now()\n",
    "    end_string = end.strftime(time_format)\n",
    "    print(\"End time: \" + str(end_string))\n",
    "    time_taken = end - start\n",
    "    print(\"Time taken: \" + str(time_taken))\n",
    "    print(\"The model 'svm_clf' may now be used to make new predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-commons",
   "metadata": {},
   "source": [
    "## Making predictions on unseen data\n",
    "\n",
    "Using the SVM classifier trained in CB7, 'svm_clf' may be used to classify unseen data.\n",
    "\n",
    "This data should ideally be structured in the same way as it was in the original BBC news article data i.e."
   ]
  },
  {
   "cell_type": "raw",
   "id": "powerful-queen",
   "metadata": {},
   "source": [
    "root directory (containing this notebook)\n",
    "    |\n",
    "    |_ datasets_coursework1\n",
    "        |\n",
    "        |_ unseen\n",
    "            |\n",
    "            |_ entertainment\n",
    "            |   |\n",
    "            |   |_ 1.txt\n",
    "            |   |_ 2.txt\n",
    "            |   |_ ...\n",
    "            |\n",
    "            |_ business\n",
    "            |   |\n",
    "            |   |_ 99.txt\n",
    "            |   |_ ...\n",
    "            |\n",
    "            |_ sport\n",
    "            |   |_...\n",
    "            |\n",
    "            |_ politics\n",
    "            |   |_ ...\n",
    "            |\n",
    "            |_ tech\n",
    "                |_ ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-florist",
   "metadata": {},
   "source": [
    "Although the classifier will still be able to run on any articles which are found within the sub-folders of the 'unseen', if the structure differs from that above, it will not be possible to measure performance metrics by simply running the CB8.\n",
    "\n",
    "In the dummy data included in the GitHub repository, 3 additional articles from the BBC news website have been added (one each within business, entertainment and policy sub-folders). These files may simply be removed to check the performance of the trained model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "arranged-karaoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.6666666666666666\n",
      "Average precision: 0.5\n",
      "Average recall: 0.6666666666666666\n",
      "Average F1-score: 0.5555555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pete/anaconda3/envs/CMT316-Coursework1/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "### Code Block 8 - Making predictions on unseen data\n",
    "\n",
    "unseen_data = get_article_data('unseen/')\n",
    "\n",
    "X_unseen, Y_unseen = xy_vector_split(categories, unseen_data, vocab)\n",
    "\n",
    "\n",
    "if set_k_best:\n",
    "    X_unseen = features.transform(X_unseen)\n",
    "    \n",
    "if set_tfidf:\n",
    "    X_unseen = tfidf.fit_transform(X_unseen)\n",
    "    \n",
    "if set_normalise:\n",
    "    X_unseen = sc.fit_transform(X_unseen)\n",
    "\n",
    "Y_unseen_predictions = svm_clf.predict(X_unseen)\n",
    "\n",
    "accuracy_unseen = accuracy_score(Y_unseen, Y_unseen_predictions)\n",
    "precision_unseen = precision_score(Y_unseen, Y_unseen_predictions, average='macro')\n",
    "recall_unseen = recall_score(Y_unseen, Y_unseen_predictions, average='macro')\n",
    "f1_unseen = f1_score(Y_unseen, Y_unseen_predictions, average='macro')\n",
    "\n",
    "print(\"Average accuracy: \" + str(accuracy_unseen))\n",
    "print(\"Average precision: \" + str(precision_unseen))\n",
    "print(\"Average recall: \" + str(recall_unseen))\n",
    "print(\"Average F1-score: \" + str(f1_unseen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
